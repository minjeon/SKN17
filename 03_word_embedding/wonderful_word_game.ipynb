{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e21696d",
   "metadata": {},
   "source": [
    "# 유사한 단어 찾기 게임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb52f63",
   "metadata": {},
   "source": [
    "1. 사전 학습된 모델 또는 적절한 데이터셋을 찾는다.\n",
    "2. 워드 임베딩 모델을 학습시킨다\n",
    "3. 단어 유사도가 0.8 이상인 A, B를 랜덤 추출한다.\n",
    "4. A, B와 대응되는 C를 추출한다.\n",
    "5. D를 입력받는다.\n",
    "\n",
    "=>\n",
    "A:B = C:D 관계에 대응하는 D를 찾는 게임을 만든다.\n",
    "ex) A: 산, B: 바다, C: 나무, D: 물"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94a2e55",
   "metadata": {},
   "source": [
    "**<출력 예시>**\n",
    "\n",
    "관계 [ 수긍 : 추락 = 대사관 : ?] <br>\n",
    "모델이 예측한 가장 적합한 단어 : 잠입 <br>\n",
    "당신의 답변과 모델 예측의 유사도 : 0.34 <br>\n",
    "아쉽네요. 더 생각해보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724bb89b",
   "metadata": {},
   "source": [
    "# 파일 한 개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "245600ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 전처리\n",
    "\n",
    "from lxml import etree      # xml, html 파일을 처리하기 위한 경량화된 라이브러리, etree : xml파일을 파싱하고 조작할 수 있는 함수 \n",
    "import re\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import json\n",
    "from konlpy.tag import Okt\n",
    "from gensim.models import Word2Vec, FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "e3156d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON 데이터의 최상위 키: dict_keys(['SJML'])\n"
     ]
    }
   ],
   "source": [
    "# BOBR210001636718.json 파일 경로\n",
    "file_path = 'data/BOBR210001636718.json'\n",
    "\n",
    "# JSON 파일 불러오기\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "# 현재 json_data의 최상위 키들을 출력해봅니다.\n",
    "print(\"JSON 데이터의 최상위 키:\", json_data.keys())\n",
    "\n",
    "# 만약 'text' 키가 없다면, 오류가 발생합니다.\n",
    "# ['header', 'text', 'SJML'] 와 같이 'text'가 포함되어 있는지 확인하세요.\n",
    "# 만약 있다면, 오류는 다른 곳에서 발생했을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "8ad58213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['이수근', '은', '너무', '화', '내다', '너무', '시끄럽다', ','],\n",
       " ['백',\n",
       "  '번',\n",
       "  '만',\n",
       "  '번',\n",
       "  '돈까스',\n",
       "  '가격',\n",
       "  '책정',\n",
       "  '은',\n",
       "  '이수근',\n",
       "  '맞다',\n",
       "  '봄',\n",
       "  '.',\n",
       "  '진짜',\n",
       "  '분명',\n",
       "  '서다',\n",
       "  '400',\n",
       "  '2만',\n",
       "  '인데',\n",
       "  '는',\n",
       "  '(',\n",
       "  '반',\n",
       "  '사회',\n",
       "  '적용',\n",
       "  ')',\n",
       "  '이네',\n",
       "  '이르다',\n",
       "  '욕',\n",
       "  '하다',\n",
       "  '사람',\n",
       "  '분명하다',\n",
       "  '생기다',\n",
       "  ','],\n",
       " ['왤케', '싸우다', '임'],\n",
       " ['그냥',\n",
       "  '소스',\n",
       "  '조그마하다',\n",
       "  '그릇',\n",
       "  '담기다',\n",
       "  '부어',\n",
       "  '먹다',\n",
       "  '찍다',\n",
       "  '먹다',\n",
       "  '알다',\n",
       "  '하다',\n",
       "  '않다',\n",
       "  ','],\n",
       " ['44만원', '식', '재료', '판매', '수익', '22만원', ',']]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = './data/BOBR210001636718.json'\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "sentences = [item['content'] for item in json_data['SJML']['text']]\n",
    "\n",
    "okt = Okt()\n",
    "def load_stopwords(file_path):\n",
    "    with open(file_path, 'r', encoding='UTF-8') as f:\n",
    "        stopwords = [line.strip() for line in f]\n",
    "    return stopwords\n",
    "ko_stopwords = load_stopwords('ko_stopwords.txt')\n",
    "\n",
    "preprocessed_data = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    #nouns = okt.morphs(sentence)\n",
    "    tokens = okt.morphs(sentence, stem=True)\n",
    "    tokens = [token for token in tokens if token not in ko_stopwords]\n",
    "    preprocessed_data.append(tokens)\n",
    "\n",
    "preprocessed_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "b9c34490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2711, 50)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FastText(\n",
    "    sentences=preprocessed_data,\n",
    "    vector_size=50,\n",
    "    window=5,\n",
    "    min_count=1, \n",
    "    sg=1\n",
    ")\n",
    "\n",
    "model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "f153ff38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('래', 0.16803424060344696),\n",
       " ('세넼', 0.1608595997095108),\n",
       " ('햇반', 0.14180752635002136),\n",
       " ('슴', 0.13784340023994446),\n",
       " ('맞네욭', 0.13614748418331146),\n",
       " ('파네', 0.13567087054252625),\n",
       " ('차차', 0.13547828793525696),\n",
       " ('천재', 0.12999187409877777),\n",
       " ('맨', 0.12988676130771637),\n",
       " ('랰', 0.12741051614284515)]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('짬뽕')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "862cd528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: 안전, B: 직전\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "all_words = list(model.wv.index_to_key)\n",
    "    \n",
    "while True:\n",
    "    a_word, b_word = random.sample(all_words, 2)\n",
    "    similarity = model.wv.similarity(a_word, b_word)\n",
    "\n",
    "    if similarity >= 0.8:\n",
    "        print(f'A: {a_word}, B: {b_word}')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "ae15195b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "관계 [안전 : 직전 = 배댕 : ?]\n",
      "\n",
      "모델이 예측한 가장 적합한 단어: 골\n",
      "당신의 답 후진 과 모델의 예측 유사도 : 0.0802\n",
      "아쉽네요. 더 생각해보세요.\n"
     ]
    }
   ],
   "source": [
    "candidate_c_words = [word for word in all_words if word not in [a_word, b_word]]\n",
    "\n",
    "c_word = random.choice(candidate_c_words)\n",
    "analogy_result = model.wv.most_similar(positive=[c_word, a_word], negative=[b_word], topn=1)\n",
    "\n",
    "predicted_d_word = analogy_result[0][0]\n",
    "\n",
    "print(f'관계 [{a_word} : {b_word} = {c_word} : ?]')\n",
    "\n",
    "user_input = input('모델이 예측한 가장 적합한 단어를 입력하세요: ').strip()\n",
    "\n",
    "print(f'\\n모델이 예측한 가장 적합한 단어: {predicted_d_word}')\n",
    "\n",
    "similarity = model.wv.similarity(user_input, predicted_d_word)\n",
    "print(f'당신의 답 {user_input} 과 모델의 예측 유사도 : {similarity:.4f}')\n",
    "if similarity < 0.5:\n",
    "    print('아쉽네요. 더 생각해보세요.')\n",
    "else:\n",
    "    print('거의 정답이예요!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020603c5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e571cf7",
   "metadata": {},
   "source": [
    "# 더 많은 파일로 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "8509ff0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "from gensim.models import FastText\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# data/media 폴더 안에 있는 json 파일 목록 가져오기\n",
    "files = glob.glob(\"data/media/*.json\")\n",
    "\n",
    "# 앞에서 100개만 사용\n",
    "sample_files = files[:100]\n",
    "\n",
    "sentences = []\n",
    "\n",
    "#for fname in sample_files:\n",
    "#    with open(fname, \"r\", encoding=\"utf-8\") as f:\n",
    "#        data = json.load(f)\n",
    "#    sentence.extend([item['content'] for item in data]['SJML']['text']) -> 오류 발생, 구조가 이렇게 안 생긴 파일도 있는듯\n",
    "\n",
    "for file in sample_files:\n",
    "    try:\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        text_list = data['SJML']['text']\n",
    "\n",
    "        # 리스트의 각 아이템을 확인\n",
    "        for item in text_list:\n",
    "            # item이 딕셔너리(dictionary) 타입이고, 'content' 키가 있는지 확인\n",
    "            if isinstance(item, dict) and 'content' in item:\n",
    "                sentences.append(item['content'])\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "# 불용어 처리\n",
    "def load_stopwords(file_path):\n",
    "    with open(file_path, 'r', encoding='UTF-8') as f:\n",
    "        stopwords = [line.strip() for line in f]\n",
    "    return stopwords\n",
    "ko_stopwords = load_stopwords('ko_stopwords.txt')\n",
    "\n",
    "preprocessed_data = []\n",
    "\n",
    "korean_stopword = re.compile('[^가-힣ㄱ-ㅎㅏ-ㅣ.,?!:;]')\n",
    "\n",
    "for sentence in sentences:\n",
    "    filtered_sentence = korean_stopword.sub('', sentence)\n",
    "    tokens = okt.morphs(filtered_sentence, stem=True)\n",
    "    tokens = [token for token in tokens if token not in ko_stopwords]\n",
    "    preprocessed_data.append(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8f61f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nouns_per_sentence = [\n",
    "#    [w for w,t in okt.pos(\" \".join(s), norm=True, stem=True) if t=='Noun']\n",
    "#    for s in preprocessed_data\n",
    "#]\n",
    "#nouns_per_sentence = [ns for ns in nouns_per_sentence if ns] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "6c8d6314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14474, 50)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FastText(\n",
    "    sentences=preprocessed_data,\n",
    "    vector_size=50,\n",
    "    window=5,\n",
    "    min_count=1, \n",
    "    sg=1\n",
    ")\n",
    "\n",
    "model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "1587820d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('고다이로', 0.9978534579277039),\n",
       " ('류현상', 0.9978373050689697),\n",
       " ('오존층', 0.9977177977561951),\n",
       " ('근무시간', 0.9976955652236938),\n",
       " ('타임머신', 0.9973708987236023),\n",
       " ('무료음원', 0.9972454309463501),\n",
       " ('돼지고기', 0.9972369074821472),\n",
       " ('위아래', 0.9972060918807983),\n",
       " ('믹스테잎', 0.9971486330032349),\n",
       " ('파이터', 0.9970571994781494)]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('짬뽕')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "f2ea6a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: 뮤직, B: 보안\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "all_words = list(model.wv.index_to_key)\n",
    "    \n",
    "while True:\n",
    "    a_word, b_word = random.sample(all_words, 2)\n",
    "    similarity = model.wv.similarity(a_word, b_word)\n",
    "\n",
    "    if similarity >= 0.8:\n",
    "        print(f'A: {a_word}, B: {b_word}')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "41f546ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "관계 [뮤직 : 보안 = 반성 : ?]\n",
      "\n",
      "모델이 예측한 가장 적합한 단어: 고명\n",
      "당신의 답 안녕 과 모델의 예측 유사도 : 0.9818\n",
      "거의 정답이예요!\n"
     ]
    }
   ],
   "source": [
    "candidate_c_words = [word for word in all_words if word not in [a_word, b_word]]\n",
    "\n",
    "c_word = random.choice(candidate_c_words)\n",
    "analogy_result = model.wv.most_similar(positive=[c_word, a_word], negative=[b_word], topn=1)\n",
    "\n",
    "predicted_d_word = analogy_result[0][0]\n",
    "\n",
    "print(f'관계 [{a_word} : {b_word} = {c_word} : ?]')\n",
    "\n",
    "user_input = input('모델이 예측한 가장 적합한 단어를 입력하세요: ').strip()\n",
    "\n",
    "print(f'\\n모델이 예측한 가장 적합한 단어: {predicted_d_word}')\n",
    "\n",
    "similarity = model.wv.similarity(user_input, predicted_d_word)\n",
    "print(f'당신의 답 {user_input} 과 모델의 예측 유사도 : {similarity:.4f}')\n",
    "if similarity < 0.5:\n",
    "    print('아쉽네요. 더 생각해보세요.')\n",
    "else:\n",
    "    print('거의 정답이예요!')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
